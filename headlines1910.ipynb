{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# News headlines in alternative universes\n",
    "Project for the  Data Jam for Newspaper Navigator library of congress.\n",
    "\n",
    "## UPDATE:\n",
    "I recommend to open directly the colab version instead of the jupyter notebook version: https://colab.research.google.com/drive/1F0FZfoS_cbi1C-vaaln-DtHW5m554z2a?usp=sharing                                                             "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Proposal: We choose a year in which a world tragedy happened. We select headlines related to that tragey and train an algorithm to learn to create fake headlines using the style and prhases of that year. The algorithm takes a user generated phrase negating the tragedy and completes it as if those tragedies didn't occur.\n",
    "\n",
    "We test with 1910, that year the mexican revolution started because Porfirio Diaz was in power for more than 31 years.\n",
    " \n",
    "    Input phrase:  'Peace in Mexico after Porfirio Diaz is removed from office.'\n",
    "\n",
    "    Fake headline from 1910: 'Peace in Mexico after Porfirio Diaz is removed from office.\\nJAPAN WILL SEND FLEET Mexico is Elated Over Pre parations Made to Cele brate Independence.'\n",
    "\n",
    "    Fake headline from 1910: 'Peace in Mexico after Porfirio Diaz is removed from office. By - the Mexican News Agency \\' River Review.\\nSTATUS OF MEXICO IMPROV ENOUGH FOR NOW El Paso Residents Opt- In to Stay put and Watch the Movements in Mexico.'\n",
    "\n",
    "\n",
    "    Input phrase:'Porfirio Diaz stepped out of office'\n",
    "    \n",
    "    Fake headline from 1910: 'Porfirio Diaz stepped out of office today, becoming President of Mexico for the fifth time'    \n",
    "\n",
    "    Input phrase:'Porfirio Diaz stepped out of office to avoid a war'\n",
    "    \n",
    "    Fake headline from 1910: 'Porfirio Diaz stepped out of office to avoid a war, but American officials were fearful that an armed conflict was imminent'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data used:\n",
    "    \n",
    "    1910_headlines.json, 1910 is the year of the Mexican Revolution\n",
    "      \n",
    "Tools used:\n",
    "    \n",
    "    python3 \n",
    "    Google colab notegook on GPT-2 (while it uses tensorflow, no tensorflow knowledge is required to reproduce this experiment).\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data collection.\n",
    "To test you can download a sample data from 'https://news-navigator.labs.loc.gov/prepackaged/1910_headlines_sample.json'\n",
    "Instructions to get the whole dataset from the same of different years can be found at https://news-navigator.labs.loc.gov\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Eric Dolores, library of congress open data\n",
    "\n",
    "year = '1910' #include the year you want to obtaine the headlines from\n",
    "assert year \n",
    "\n",
    "os.system(f'wget https://news-navigator.labs.loc.gov/prepackaged/{year}_headlines.json')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once the json file is downloaded we need:\n",
    "    \n",
    "    the path to that json file\n",
    "    keys: a list of topics we are interested in. \n",
    "    badWord/badPairOfWords: a word or a pair of words that may be included due to the similarity to the topics but is/are unrelated, we will use it to remove non related headlines\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# include the place where the json is saved in your local computer\n",
    "pathToJsonHeadlines = f'{year}_headlines.json' #change this by the name/location of the file you downloaded\n",
    "keys = ['Mexico','Mexican'] # modify this to include the list of topics, if not topics are given you will work with the whole year of data\n",
    "# you can include at most one of the next two variables or write your custome removal code for several unrelated words\n",
    "pairOfBadWords ='New Mexico'\n",
    "badWord = ''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "The following code:\n",
    "\n",
    "- reads the headlines\n",
    "- filters by the keys\n",
    "- removes unrelated headlines\n",
    "- saves into a file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " Eric Dolores, library of congress open data\n",
    "\n",
    "import json\n",
    "\n",
    "assert pathToJsonHeadlines #include the place where the json is saved in your local computer\n",
    "\n",
    "with open(pathToJsonHeadlines,'r') as f:\n",
    "    jsonData = json.load(f)\n",
    "    \n",
    "print(f'sample dictionary:\\n {jsonData[0]}\\n') \n",
    "\n",
    "def pairRemoval(allHeadlines, badPairOfWords):\n",
    "    filtered = []\n",
    "    begining = badPairOfWords.split(' ')[0]\n",
    "    end = badPairOfWords.split(' ')[1]\n",
    "    for stuff in allHeadlines:\n",
    "        for i, item in enumerate(stuff['ocr']):\n",
    "            if item == end and i>0 and stuff['ocr'][i-1] ==begining:\n",
    "                break\n",
    "        filtered.append(stuff)\n",
    "    print(f'we removed those headlines containing {badPairOfWords} and now we have {len(filtered)} headlines \\n')\n",
    "    return filtered\n",
    "\n",
    "def wordRemoval(allHeadlines, badWord):\n",
    "    filtered = []\n",
    "    for stuff in allHeadlines:\n",
    "        if item in stuff['ocr']:\n",
    "                continue\n",
    "        filtered.append(stuff)    \n",
    "    print(f'we removed those headlines containing {badWord} and now we have {len(filtered)} headlines \\n')\n",
    "    return filtered\n",
    "\n",
    "\n",
    "print('We the data and keep  those that use the key words, if any.')\n",
    "if keys:\n",
    "\n",
    "  # filter by the keywords\n",
    "  relatedHeadlines = []\n",
    "  for element in jsonData:\n",
    "      for key in keys:\n",
    "          if key in element['ocr']:\n",
    "              relatedHeadlines.append(element)\n",
    "              break\n",
    "            \n",
    "  print(f'we found {len(relatedHeadlines)} headlines having the key words {keys} \\n')\n",
    "\n",
    "  # We filter for wrong headlines, perhaps you want Mexico but not New Mexico\n",
    "else:\n",
    "  relatedHeadlines = jsonData\n",
    "\n",
    "print('Now we remove words related to topics we want to avoid')\n",
    "if pairOfBadWords:\n",
    "    filtered = pairRemoval(relatedHeadlines, pairOfBadWords) # I needed it to remove 'New Mexico' while looking for the country 'Mexico'\n",
    "elif badWord:\n",
    "    filtered = wordRemoval(relatedHeadlines, badWord)\n",
    "else:\n",
    "    filtered = relatedHeadlines\n",
    "\n",
    "\n",
    "print(f'sample filtered input:\\n {filtered[0]}') \n",
    "\n",
    "#We purpusedly delayed the construction of strings in case the metadata is useful\n",
    "strings = ''\n",
    "for item in filtered:\n",
    "    prhase = ' '.join(item['ocr'])\n",
    "    strings = strings + prhase+' \\n '\n",
    "\n",
    "# We collect those strings\n",
    "filename=pathToJsonHeadlines.replace('json','txt')    \n",
    "\n",
    "with open(filename,'w') as f:\n",
    "    f.write(strings)     \n",
    "print(f\"We saved the input into the file {filename}\")\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "The next step could be done locally using https://github.com/minimaxir/gpt-2-simple but I think for economical reasons and to let the audience experiment, I prefer to use a google colab.\n",
    "\n",
    "You basically have to run everything and just input the file we created to obtain sentences. While you can do this locally, using colab allows you to use GPU for free.\n",
    "\n",
    "'https://colab.research.google.com/drive/1VLG8e7YSEwypxU-noRNhsv5dW4NfTGce'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    \n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conclusions and future ways to continue this project:\n",
    "This project was born on the Data Jam for Newspaper Navigator library of congress. I realized early that training was going to consume most of the time.\n",
    "\n",
    "This is very biased data,  a revolution started in 1910 so random strings are related to war in Mexico:\n",
    "\n",
    "    NO PEACE FOR MEXICO CITY Government to Be Quieter in the Postponed Marshal Correa. President Too Escapes to Attend United Nations Conference. \n",
    " \n",
    "\n",
    "Even if we give a guideline, the algorithm may not return a headline with the output we aim:\n",
    "\n",
    "\n",
    "    Input phrase:'Porfirio Diaz stepped out of office'\n",
    "\n",
    "    Fake headline from 1910: 'Porfirio Diaz stepped out of office today, becoming President of Mexico for the fifth time' \n",
    "\n",
    "\n",
    "In fact the algorithm returns a headline given an arbitrary input.\n",
    "\n",
    "    Input phrase: 'Burritos are declared a danger to society '\n",
    "\n",
    "    Fake headline from 1910: 'Burritos are declared a danger to society and the state militia is sent out to quell the uprising.' \n",
    "\n",
    " \n",
    " \n",
    "Since we have a dataset of images of 1910, one possible continuation of this project is to assign an image to the fake headline and return a fake news paper first plane. I haven't found a natural way to associate a picture file to an arbitrary string but as soon as a good enough solution occurs I'll write that part.\n",
    " \n",
    " \n",
    " \n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "We rely on the work of Max Woolf http://minimaxir.com/ and the data from the Library of Congress, Newspaper Navigator dataset: Extracted Visual Content from Chronicling America. https://news-navigator.labs.loc.gov"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
